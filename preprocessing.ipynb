{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pre-processing MultiplEYE Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide through how to process the eye-tracking data and the psychometric tests data collected within the MultiplEYE project. This goal of this notebook is twofold:\n",
    "\n",
    "1. To provide a step-by-step guide on how to preprocess MultiplEYE data using the `pymovements` library and our custom preprocessing functions.\n",
    "2. To serve as a tutorial for researchers who want to preprocess their own MultiplEYE data, or data from other eye-tracking datasets, using the `pymovements` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Preparation steps\n",
    "1. Download the data folder from the online repository. Note that this is only possible if you have access to at least one data collection protected folder. You will have access if you are an active member of one data collection group. Download the entire content of the folder.\n",
    "When you download it from SwitchDrive, it will automatically create a .tar file.\n",
    "2. Add the folder to the `data/` folder in this repo. The name of the folder is the data collection name, e.g., `MultiplEYE_ZH_CH_Zurich_1_2025`.\n",
    "3. Extract the .tar file in the `data/` folder.\n",
    "4. Make sure that the folder structure is correct. It should look like the one online and like this (there might be more data but this is not relevant at this point):\n",
    "```\n",
    "\tMultiplEYE_ZH_CH_Zurich_1_2025/\n",
    "\t\tdocumentation/\n",
    "\t\teye-tracking-sessions/\n",
    "\t\t\t001_.../\n",
    "\t\t\t002_.../\n",
    "\t\t\t...\n",
    "\t\t\tpilot_sessions/\n",
    "\t\t\t\t001_.../\n",
    "\t\t\t\t002_.../\n",
    "\t\t\t\t...\n",
    "\t\tpsychometric-tests-sessions/\n",
    "\t\tstimuli_MultiplEYE_ZH_CH_Zurich_1_2025/\n",
    "\t\t...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## The config file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The pipeline uses a config file which can be used to specify parameters and settings for the preprocessing. It is located in the top repo folder and named like this: `multipleye_settings_preprocessing.yaml`. Please fill in the file with the appropriate settings for your data collection. The file is well documented, so you can find explanations for each parameter there. Once you did so you can run the next cell.\n",
    "\n",
    "Please restart the notebook kernel and re-run the cell whenever you make changes to the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocessing.data_collection.multipleye_data_collection import prepare_language_folder\n",
    "from preprocessing.data_collection.multipleye_data_collection import (\n",
    "    MultipleyeDataCollection,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "import preprocessing\n",
    "\n",
    "# the config will be loaded into general constants module, so we can access all settings at the same place\n",
    "from preprocessing import constants\n",
    "from preprocessing.scripts.prepare_language_folder import prepare_language_folder\n",
    "\n",
    "from preprocessing.metrics.words import (\n",
    "    all_tokens_from_aois,\n",
    "    mark_skipped_tokens,\n",
    "    repair_word_labels,\n",
    ")\n",
    "from preprocessing.metrics.fixations import annotate_fixations\n",
    "from preprocessing.metrics.reading_measures import build_word_level_table\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data collection name from the config and create the path to the data folder\n",
    "this_repo = Path().resolve()\n",
    "data_collection_name = constants.DATA_COLLECTION_NAME\n",
    "data_folder_path = this_repo / \"data\" / data_collection_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## MultiplEYE-specific preprocessing & cleaning\n",
    "\n",
    "In order to be able to run a more generic preprocessing, the MultiplEYE data folder for one language needs to be cleaned and organized in a specific way. Running the script below will:\n",
    "- unzip session folders if needed\n",
    "- move session folders from core_sessions folder to the top folder\n",
    "- check if there is a config file in the stimuli folder (if not, the stimulus folder was probably not uploaded correctly)\n",
    "- check if there are psychometric tests (if applicable)\n",
    "\t- if necessary, restructure the psychometric test folder.\n",
    "\n",
    "These steps are very individual for this data collection and results from bugs or changes across the years of collecting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the preparation function to prepare the language folder structure\n",
    "prepare_language_folder(data_collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Next, we create a `MultipleyeDataCollection` object from the data folder. This will allow us to easily access the sessions and their information in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipleye = MultipleyeDataCollection.create_from_data_folder(\n",
    "    data_folder_path,\n",
    "    include_pilots=constants.INCLUDE_PILOTS,\n",
    "    excluded_sessions=constants.EXCLUDE_SESSIONS,\n",
    "    included_sessions=constants.INCLUDE_SESSIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Stage 0: Converting EDF to ASC and Preparing Session-Level Information\n",
    "\n",
    "Stage 0 refers to the initial steps of preprocessing, which involve converting raw eye-tracking data from its original format (e.g., EDF) into a more accessible format (e.g., ASC), and preparing session-level information. This stage is specific to EyeLink eye-trackers and can be omitted for other eye-trackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipleye.convert_edf_to_asc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Once this conversion has been completed, we can load all sessions and parse the .asc files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipleye.prepare_session_level_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print an overview on the data collection and the sessions\n",
    "multipleye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Stage 1: Extracting Gaze Samples\n",
    "\n",
    "In the first preprocessing stage, we extract gaze samples from the .asc files and create a gaze dataframe for each session. This dataframe contains the raw gaze data, including the x and y coordinates of the gaze, the timestamp. We also save the raw gaze data in a separate file for each session.\n",
    "\n",
    "The next steps are performed for one session only. It is always possible to loop over all sessions and apply the same preprocessing steps to each of them, but for the sake of clarity and simplicity, we will work with one session as an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick only one session as an example to work with in the next steps\n",
    "sessions = [s for s in multipleye]\n",
    "sess = sessions[0]\n",
    "idf = sess.session_identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Creating Gaze Frame from ASCII File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the path to the .asc file for the session\n",
    "asc = sess.asc_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze = preprocessing.load_gaze_data(\n",
    "    asc_file=asc,\n",
    "    lab_config=sess.lab_config,\n",
    "    session_idf=idf,\n",
    "    trial_cols=constants.TRIAL_COLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save gaze and metadata\n",
    "preprocessing.save_raw_data(constants.OUTPUT_DIR, idf, gaze)\n",
    "preprocessing.save_session_metadata(constants.OUTPUT_DIR, idf, gaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "In order to have the metadata which is extracted by pymovements available to create out session overview, we get this information from pymovements and store it in our session object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the ._metadata like this is just a temporary solution, this will be changed soon\n",
    "sess.pm_gaze_metadata = gaze._metadata\n",
    "sess.calibrations = gaze.calibrations\n",
    "sess.validations = gaze.validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Coordinate and Velocity Preprocessing\n",
    "\n",
    "Eye movements are recorded in screen pixel coordinates, which depend on stimulus size and monitor setup. To compare gaze behavior across participants, screens, or datasets, it is standard to convert pixel positions \n",
    "into **degrees of visual angle (dva)**. Next, we compute **gaze velocity**, which allows us to detect saccades and distinguish them from fixations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the gaze samples\n",
    "gaze.samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.preprocess_gaze(gaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the preprocessed gaze samples, the dataframe should now also contain a position in dva and velocity columns\n",
    "gaze.samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Stage 2a: Detect Events and Compute Their Properties\n",
    "\n",
    "Eye-tracking data are typically segmented into events, i.e. `fixations` and `saccades`. Fixations represent moments when the eyes remain relatively still, allowing visual information to be processed, while saccades are the rapid movements between fixations that reposition the gaze. Detecting these events and computing their properties, such as `dispersion`, fixation `duration`, saccade `amplitude`, and `peak velocity`, provides the foundation for analyzing visual behavior and understanding how participants explore a stimulus.\n",
    "\n",
    "### Fixations\n",
    "\n",
    "We can detect fixations by applying the `I-VT` or the `I-DT` method.\n",
    "\n",
    "The **I-VT (Velocity-Threshold Identification)** method distinguishes fixation and saccade points based on their point-to-point velocities. Each point is classified as a fixation if its velocity is below the specified threshold. Consecutive fixation points are then merged into a single fixation. A threshold of 20 degrees/second is commonly used as a default maximum value. Read more about [the IVT algorithm in the documentation](https://pymovements.readthedocs.io/en/stable/reference/api/pymovements.events.detection.ivt.html) \n",
    "\n",
    "The **I-DT (Dispersion-Threshold Identification)** method finds fixations by grouping consecutive points within a maximum separation (dispersion) threshold and a minimum duration threshold. The algorithm slides a moving window across the data: if the dispersion within the window is below the threshold, the window represents a fixation and is gradually expanded until the dispersion exceeds the threshold.\n",
    "Read more about [our implementation of the IDT method](https://pymovements.readthedocs.io/en/stable/reference/api/pymovements.events.detection.idt.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "We use the `I-VT` algorithm with the following key deafault parameters:\n",
    "- `minimum duration`: 100 ms \n",
    "- `velocity threshold`: 20.0\n",
    "\n",
    "Such properties as `location`, containing the centroid coordinates of each fixation, and `dispersion` will also be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.detect_fixations(\n",
    "    gaze,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Saccades\n",
    "\n",
    "Saccades are rapid eye movements that shift the point of fixation from one location to another. We detect saccades (or micro-saccades) from the velocity sequence of gaze data using the [microsaccades algorithm](https://pymovements.readthedocs.io/en/stable/reference/api/pymovements.events.detection.microsaccades.html#pymovements.events.detection.microsaccades). This algorithm implements a noise-adaptive velocity threshold, meaning that the detection threshold automatically scales with the noise level of the velocity signal. Such properties as `amplitude` and `peak velocity` of the detected saccades will also be calcuated.\n",
    "\n",
    "The key default parameters are:\n",
    "- `threshold_factor`: Multiplier used to determine the velocity threshold relative to the noise level of the signal. The default value is 6. A higher factor makes the algorithm more conservative (detects fewer saccades), while a lower factor makes it more sensitive.\n",
    "- `minimum_duration`: Defines how long a velocity peak must persist to be classified as a saccade. The duration is expressed in the same units as timesteps. If no timesteps are provided, the value refers to the number of samples (default = 6), which corresponds to about 12 ms at a 500 Hz sampling rate. Shorter events are ignored as noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.detect_saccades(\n",
    "    gaze,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Save our events data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.save_events_data(\n",
    "    constants.FIXATION,\n",
    "    constants.OUTPUT_DIR,\n",
    "    idf,\n",
    "    split_column=\"trial\",\n",
    "    name_columns=[\"trial\", \"stimulus\"],\n",
    "    file_columns=[\"onset\", \"duration\", \"location_x\", \"location_y\", \"page\"],\n",
    "    data=gaze,\n",
    ")\n",
    "\n",
    "preprocessing.save_events_data(\n",
    "    constants.SACCADE,\n",
    "    constants.OUTPUT_DIR,\n",
    "    idf,\n",
    "    split_column=\"trial\",\n",
    "    name_columns=[\"trial\", \"stimulus\"],\n",
    "    file_columns=[\n",
    "        \"onset\",\n",
    "        \"duration\",\n",
    "        \"amplitude\",\n",
    "        \"peak_velocity\",\n",
    "        \"dispersion\",\n",
    "        \"page\",\n",
    "    ],\n",
    "    data=gaze,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Stage 2b: Map Fixations to AOIs\n",
    "\n",
    "Once we have the fixations, we can map each of them to the AOIs of the stimulus. The resulting scanpath can then be saved. Note that this features is not yet completely finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.map_fixations_to_aois(gaze, sess.stimuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting mapping can be stored as a scanpath, which is a sequence of AOIs that were fixated in the order they were fixated.\n",
    "preprocessing.save_scanpaths(constants.OUTPUT_DIR, idf, gaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata again\n",
    "preprocessing.save_session_metadata(constants.OUTPUT_DIR, idf, gaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Stage 3: Calculate AOI-based Measures\n",
    "\n",
    "In this last step, we calculate the aoi-based measures. These are also refered to as reading measures, as they are typically used in reading research. They include measures such as first pass fixation duration (FPF), total fixation count (TFC), regression path duration (RPD), and many more. These measures are calculated based on the fixations that were mapped to the AOIs in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we pick just one stimulus of our session as an example\n",
    "stimulus = 4\n",
    "trial_label = sess.stimuli[stimulus].trial_id\n",
    "aois = sess.stimuli[stimulus].text_stimulus.aois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add word label to blank spaces between words in AOIs. This step is necessary as the AOIs files currently map white space to the preceding word, however, it should be mapped to the word following the white space.\n",
    "aois_clean = repair_word_labels(aois)\n",
    "\n",
    "# collect all words from AOIs for the given trial\n",
    "all_tokens = all_tokens_from_aois(aois_clean, trial=trial_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Fixation-based Metrics\n",
    "\n",
    "As an intermediate step, the fixations are annotated. These annoataions include:\n",
    "- The run ID. This ID specifies continuous sequences of fixations on the same word. It is used to calculate first pass and second pass measures.\n",
    "- Whether the fixation is within the first pass or not\n",
    "- The index of the preceding word and the following word\n",
    "- If the saccade entering or leaving the fixation is a regression or not\n",
    "- Whether it is the first fixation on the word or not\n",
    "\n",
    "This information is necessary to calculate the reading measures in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fixation table\n",
    "fixation_table = annotate_fixations(gaze.events.frame)\n",
    "fixation_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  annotate skipped words based on fixation table and all tokens\n",
    "words_with_skip = mark_skipped_tokens(all_tokens, fixation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate word-level reading measures\n",
    "word_level_table = build_word_level_table(\n",
    "    words=words_with_skip,\n",
    "    fix=fixation_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=50):\n",
    "    print(\n",
    "        word_level_table.filter(pl.col(\"page\") == \"page_1\").select(\n",
    "            [\n",
    "                \"word_idx\",\n",
    "                \"word\",\n",
    "                \"skipped\",\n",
    "                \"FPF\",\n",
    "                \"TFC\",\n",
    "                \"SL_in\",\n",
    "                \"RPD_inc\",\n",
    "                \"RBRT\",\n",
    "                \"TFT\",\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## Final Steps\n",
    "\n",
    "In the very end, we can create the session and dataset overview and store them as well. In addition, the participant data can be parsed and stored.\n",
    "\n",
    "For the MultiplEYE data, there is also the option to create a sanity check report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipleye.create_sanity_check_report(\n",
    "    gaze,\n",
    "    sess.session_identifier,\n",
    "    output_dir=constants.OUTPUT_DIR,\n",
    "    plotting=True,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipleye.create_session_overview(sess.session_identifier, path=constants.OUTPUT_DIR)\n",
    "multipleye.create_dataset_overview(path=constants.OUTPUT_DIR)\n",
    "multipleye.parse_participant_data(constants.OUTPUT_DIR / \"participant_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.psychometric_tests.preprocess_psychometric_tests import (\n",
    "    preprocess_all_sessions,\n",
    ")\n",
    "\n",
    "preprocess_all_sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pm-user",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
